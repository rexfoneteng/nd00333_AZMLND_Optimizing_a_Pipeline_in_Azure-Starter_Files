# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary

The project use the **Bank Marketing Dataset** which contain basic information of possible clients including age, job, marital status, etc. The goal is to apply Machine Learning algorithm to estimate the likelihood of a person to become a clients those will subcribe to term deposit (final column "y") based on basic information.

In this project, I tried two approaches to deduce the models, the metric used is Classification Accuracy. First, I tried tune the hyperparamter with HyperDrive which stick with the LogisticRegression (see train.py). The obtained model was decent, with accuracy is 0.906. Next, I tried using AutoML to automate the algorithm selection and tuning the hyperparameter. The obtained is more robust with the best accuracy obtained is 0.9178 delivered by VotingEnsemble.


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The pipeline begins by retrieving the bank marketing dataset, followed by data transformation steps that address missing entries, convert categorical features to numeric representations, and transform string values into integer format before training a Logistic Regression model within the `train.py` script.

**What are the benefits of the parameter sampler you chose?**

RandomParameterSampling handle the exploaration of hyperparamer space by randomly select grid of parameter values. This scheme eliminate the exhaustive trial-and-error test on all possible grids, and hence, is computationally efficient.

**What are the benefits of the early stopping policy you chose?**

The early stopping policy help terminate the poorly performing job early. This policy couple with "random sampling" of hyperparamter search significantly enhance the efficiency of the training job.
## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

AutoML automatically deduced a VotingEnsemble as the best performing algorithm after testing multiple classification algorithms. The final model achieved 91.8% accuracy. The computing cost and model selection were efficiently managed using key configuration settings including `experiment_timeout_minutes` for time constraints, `primary_metric` for optimization goals, `n_cross_validations` for robust model evaluation. 

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

Between the two pipeline approaches, AutoML convincingly demonstrate its robustness as the primary metric of all the selected algorithms are consistently higher (normally larger than 0.91) than sticking with Logistic Regression + HyperDrive parametertuning (Accuracy: 0.906). In HyperDrive, user stick with a algorithm which may root his/her biased preference, meanwhile, AutoML will visit a selection of algorithms and try to get the best out of it.
## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

- **Feature Engineering**: Pay attention to feature that may correlate to each other, remove those correlated features can help improve the results.
- **Class Imbalance Handling**: Consider applying techniques like class re-weighting, upsampling, downsampling to avoid the imbalance in the target column.
- **Extended Search Space**: Consider increase `experiment_timeout_minutes` and expand hyperparameter ranges in HyperDrive to allow more thorough exploration, potentially discovering better parameter combinations that weren't found within the 30-minute constraint.
- **Advanced Sampling Methods**: Use Bayesian optimization to more efficiently navigate the hyperparameter space and converge faster to optimal solutions.
- **Alternative Metrics**: Focus on business-relevant metrics like precision, recall, or F1-score rather than just accuracy, since false positives/negatives have different costs in marketing campaigns.

These suggestions would help capture more data patterns in real-world data challenges, and more oriento to business objectives.
## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
![image](https://github.com/user-attachments/assets/96e37a42-6f71-4c4e-9217-e532ce0383a7)

